model:
  model_name: Qwen/Qwen2.5-0.5B-instruct #"Qwen/Qwen2.5-Math-1.5B-Instruct" #"Qwen/Qwen2.5-3B-Instruct"
  gpu_memory_utilization: 0.5
  max_seq_length: 1536
  lora_rank: 64
  load_in_4bit: true
  fast_inference: true # use_vllm
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  random_state: 42
wandb:
  enable: true
  project_name: "SMCGRPO"
  run_name: debug #"Instruct-GSM8K-16-16-Run1"
rl:
  algorithm: "SMCGRPO"
  dataset: "math"
  max_prompt_length: 512
  max_completion_length: 1024
  num_generations: 8
  max_steps: 2
  save_strategy: "time"
  save_steps: 100
  save_interval: 1800
  max_time: .inf
  beta: 0.0
  epsilon: 0.2
  epsilon_high: #0.28 # one sided
  delta: #1.5 # two sided
  loss_type: 'grpo' # 'bnpo', 'dr_grpo'
  mask_truncated_completions: # True #DAPO
  resume_from_checkpoint:
optim:
  learning_rate: 5e-6
  lr_scheduler_type: "constant"
  # use_vllm: false # TODO: optimize on vllm
  optim: "adamw_8bit" #TODO: muon optimizer
  adam_beta1: 0.9
  adam_beta2: 0.99
  weight_decay: 0.1
  warmup_ratio: 0.01
  temperature: 0.9
  max_grad_norm: 0.1
  per_device_train_batch_size: 128 # We now expect `per_device_train_batch_size` to be a multiple of `num_generations`
  gradient_accumulation_steps: 1
  logging_steps: 1
smc:
  use_smc: true
  step_token: "\n\n"
  stop_token: "\\boxed"
  max_steps: 32
  # tokens_per_step: 16  # leave commented to use step_token mode
  smc_beta: 1.0 #TODO: later use for kl term
  smc_confidence_eta: 1.0 #TODO: temperature로 교체
  smc_confidence_window_size: 1024
  smc_topk: 20
  return_all: false
  confidence:
    scoring: "entropy" #logprob, prob
    group: "mean" #geo
    aggregation: "current" # prod, min, mean, current
prm: 
  use_prm: true
  model_name: Qwen/Qwen2.5-Math-PRM-7B
  aggregation: "model" #mean, last, min
  gpu_memory_utilization: 0.8
  cuda: 5

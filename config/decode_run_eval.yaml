defaults:
  - _self_

base_model: "Qwen/Qwen2.5-0.5B"

run_name: "Original_BASE_0.5B_512_N8_D8"
checkpoints_dir: "checkpoints"
results_root: "results/decode_run"
include_base: false
skip_existing: true
save_per_checkpoint: true

model:
  model_name: ${base_model}
  tokenizer_name: ${base_model}
  gpu_memory_utilization: 0.7
  max_seq_length: 1024
  load_in_4bit: true
  fast_inference: true

datasets:
  - "amc23"
  - "gsm8k"
  - "math500"
split: "test"
num_samples: -1
max_prompt_length: 512

eval:
  repeat_cnt: 1
  sample_cnt: 5
  batch_size_groups: 32
  num_generations: 8
  max_new_tokens: 512
  pass_k: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
  run_default: false
  run_custom: true
  default_use_vllm: true

default_decode:
  do_sample: true
  temperature: 0.9
  top_p: 1.0
  top_k: 0

custom_decode:
  use_smc: true
  step_token: "\n\n"
  stop_token: "\\boxed"
  max_steps: 1000
  smc_confidence_eta: 1.0
  smc_confidence_window_size: 512
  smc_topk: 1
  return_eos: true
  return_all: false
  return_all_limit_per_group: null
  random_sampling: false
  confidence:
    scoring: "prob"
    cdf_alpha: 0.25
    group: "geo"
    aggregation: "last"

prm:
  use_prm: false
  model_name: "Qwen/Qwen2.5-Math-PRM-7B"
  aggregation: "prod"
  gpu_memory_utilization: 1.0
  cuda: 5

wandb:
  enable: true
  project_name: "SMC_Inference"
  run_name: "decode"
  group: null
  entity: null

hydra:
  run:
    dir: .
  output_subdir: null
  job:
    chdir: false

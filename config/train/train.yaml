defaults:
  - _self_
  - experiment: ???
model:
  model_name: Qwen/Qwen2.5-Math-1.5B
  gpu_memory_utilization: 0.3
  max_seq_length: 1024
  lora_rank: 64
  load_in_4bit: true
  fast_inference: true # use_vllm
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  random_state: 42
wandb:
  enable: true
  project_name: "rlvr-distill"
  run_name: debug #"Instruct-GSM8K-16-16-Run1"
rl:
  dataset: "math"
  max_prompt_length: 512
  max_completion_length: 512 #TODO: debugging temporarily
  num_generations: 16
  use_vllm: ${model.fast_inference}
  vllm_mode: "colocate"
  vllm_gpu_memory_utilization: ${model.gpu_memory_utilization}
  vllm_tensor_parallel_size: 1
  max_steps: 8000
  save_strategy: "steps" #"steps"
  save_steps: 100
  save_interval: 100 #1800 # 1800
  max_time: .inf
  mask_truncated_completions: # True #DAPO
  resume_from_checkpoint:
  teacher_model:
  teacher_value_beta:
  teacher_device:
  teacher_lora_path:
optim:
  learning_rate: 5e-6
  lr_scheduler_type: "constant"
  optim: "adamw_8bit"
  adam_beta1: 0.9
  adam_beta2: 0.99
  weight_decay: 0.1
  warmup_ratio: 0.01
  temperature: 0.9
  max_grad_norm: 0.1
  per_device_train_batch_size: 16 # We now expect `per_device_train_batch_size` to be a multiple of `num_generations`
  gradient_accumulation_steps: 1
  logging_steps: 1

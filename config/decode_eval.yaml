model:
  model_name: "Qwen/Qwen2.5-0.5B"
  gpu_memory_utilization: 0.5
  max_seq_length: 1024
  load_in_4bit: true
  fast_inference: false

wandb:
  enable: true
  project_name: "SMC_DECODE_EVAL_DEBUG"
  run_name: "Qwen2.5-0.5B"

datasets:
  # - "amc23"
  # - "gsm8k"
  - "math500"

split: "test"

eval:
  repeat_cnt: 1
  sample_cnt: 5
  # Number of prompt groups per batch (each group produces multiple trajectories)
  batch_size_groups: 64
  # Unified number of trajectories per prompt for both default/custom decoding
  num_generations: 16
  max_new_tokens: 512
  # Which decoding modes to run
  run_default: false
  run_custom: true
  # Use Unsloth+vLLM backend for default decoding
  default_use_vllm: true

default_decode:
  do_sample: true
  temperature: 0.9
  top_p: 1.0
  top_k: 0

custom_decode:
  use_smc: true
  # Sampling controls used inside SMC token selection
  do_sample: true
  temperature: 0.9
  top_p: 1.0
  top_k: 0
  # SMC-specific controls
  smc_beta: 1.0
  smc_warmup_tokens: 100
  smc_confidence_eta: 1.0
  smc_resample_threshold: 0.5 # ess threshold
  smc_confidence_window_size: 1024
  smc_topk: 20


# Default decoding only:
# uv run python evaluate-decode.py eval.run_default=true eval.run_custom=false wandb.run_name=decode-default
# Custom SMC decoding only:
# uv run python evaluate-decode.py eval.run_default=false eval.run_custom=true wandb.run_name=decode-smc

model:
  model_name: "Qwen/Qwen2.5-1.5B-Instruct"
  gpu_memory_utilization: 0.8
  max_seq_length: 1536
  load_in_4bit: true
  fast_inference: true

wandb:
  enable: true
  project_name: "SMC_Inference"
  run_name: "debug"

datasets:
  # - "amc23"
  # - "gsm8k"
  - "math500"

split: "test"
num_samples: -1 # set to -1 to use full eval set
max_prompt_length: 512 # set to -1 to keep all prompts

eval:
  repeat_cnt: 1
  sample_cnt: 5
  # Number of prompt groups per batch (each group produces multiple trajectories)
  batch_size_groups: 512
  num_generations: 64
  max_new_tokens: 1024
  # Values <= 1.0 are interpreted as proportions (pass@(proportion)); larger values use absolute k
  pass_k: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
  # Which decoding modes to run
  run_default: false
  run_custom: true
  default_use_vllm: true

default_decode:
  do_sample: true
  temperature: 0.9
  top_p: 1.0
  top_k: 0

custom_decode:
  use_smc: true
  step_token: "\n\n"
  stop_token: "\\boxed"
  max_steps: 1000
  # tokens_per_step: 16  # leave commented to use step_token mode
  smc_confidence_eta: 1.0 #TODO: temperature로 교체
  smc_confidence_window_size: 1
  smc_topk: 20
  return_eos: true
  return_all: false
  confidence:
    scoring: "entropy" #"entropy" #logprob, prob
    group: "mean" #"mean" #geo
    aggregation: "mean" # prod, min, mean, last

prm: 
  use_prm: false
  model_name: Qwen/Qwen2.5-Math-PRM-7B # Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B #
  aggregation: "prod" #last, min, prod, model
  gpu_memory_utilization: 1.0
  cuda: 5


# Default decoding only:
# uv run python evaluate-decode.py eval.run_default=true eval.run_custom=false wandb.run_name=decode-default
# Custom SMC decoding only:
# uv run python evaluate-decode.py eval.run_default=false eval.run_custom=true wandb.run_name=decode-smc

model:
  model_name: Qwen/Qwen2.5-1.5B-instruct
  gpu_memory_utilization: 0.5
  max_seq_length: 1024
  lora_rank: 64
  load_in_4bit: true
  fast_inference: true
  target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
  random_state: 42
wandb:
  enable: false
  project_name: SMCGRPO_MAIN
  run_name: Original_3B_512
rl:
  algorithm: SMCGRPO
  dataset: math
  max_prompt_length: 512
  max_completion_length: 512
  num_generations: 16
  max_steps: 8000
  save_strategy: steps
  save_steps: 100
  save_interval: 100
  max_time: .inf
  beta: 0.0
  epsilon: 0.2
  epsilon_high: null
  delta: null
  loss_type: grpo
  mask_truncated_completions: null
  resume_from_checkpoint: null
optim:
  learning_rate: 5.0e-06
  lr_scheduler_type: constant
  optim: adamw_8bit
  adam_beta1: 0.9
  adam_beta2: 0.99
  weight_decay: 0.1
  warmup_ratio: 0.01
  temperature: 0.9
  max_grad_norm: 0.1
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  logging_steps: 1
smc:
  use_smc: true
  step_token: '


    '
  stop_token: \boxed
  max_steps: 1000
  smc_confidence_eta: 1.0
  smc_confidence_window_size: 32
  smc_topk: 1
  return_eos: true
  return_all: true
  drop_reward_scheme: progress
  drop_reward_coef: 0.1
  return_all_limit_per_group: 128
  confidence:
    scoring: prob
    cdf_alpha: 0.25
    group: geo
    aggregation: last
prm:
  use_prm: false
  model_name: Qwen/Qwen2.5-Math-PRM-7B
  aggregation: model
  gpu_memory_utilization: 1.0
  cuda: 1
